<p align="center">
  <img src="docs/images/logo.png" alt="H-DaC Logo" width="300">
</p>

# Head-Twitch Response Detection & Classification Tool

A comprehensive desktop application for detecting and analyzing Head-Twitch Responses (HTRs) in rodent behavioral videos using SLEAP pose-tracking data and machine learning. This tool provides a structured workflow from parameter tuning to model deployment.

---

## Prerequisites

**Software:**
- Python 3.9 or higher
- Windows 10/11, macOS 10.14+, or Linux

**Data Requirements:**
- **Pose tracking data:** SLEAP-generated H5 files (`.h5` format)
  - *Note: Support for other pose estimation formats (DeepLabCut, etc.) planned for future releases*
- **Camera view:** Top-down view of the animal required
- **Bodypart labels:** Videos must be tracked with these 5 specific keypoints:
  - Left Ear, Right Ear, Nose, Head, Back


<p align="center">
  <img src="docs/images/rat_labels_overhead_ex2.png" alt="Required Bodypart Labels" width="200">
  <br>
  <em>Required bodypart label placement (overhead view)</em>
</p>

---

## Quick Install

```bash
git clone https://github.com/GraysonButcher/Headtwitch_Detection_and_Classification.git
cd Headtwitch_Detection_and_Classification
python -m venv htr_env
htr_env\Scripts\activate  # On Windows
pip install -e .
hdac  # Launch the application
```

ðŸ‘‰ **[Full installation guide with all options â†’](docs/installation.md)** (conda, manual setup, troubleshooting)

---

## Using H-DaC

### Scenario 1: [Ready to Predict HTRs](docs/workflow.md#deployment-fresh)
(Choose this scenario if you already have tuned detection parameters and a trained model) 
1. **(Jump to) New Project** â†’ [Deployment Guide: Fresh Start](docs/workflow.md#deployment-fresh)
2. **(Jump to) Existing Project** â†’ [Deployment Guide: Incremental Analysis](docs/workflow.md#deployment-incremental)

### Scenario 2: [Need to Prepare Your Detection System](docs/workflow.md#prepare-data)
(Choose this scenario if you're starting completely fresh or haven't finished tuning
parameters, labeling data, or training a model)
1. **(Jump to) Tune detection parameters** â†’ [Parameter Tuning Guide](docs/parameter_tuning_guide.md)
2. **(Jump to) Label ground truth data** â†’ [Ground Truth Labeling Guide](docs/workflow.md#prepare-data)
3. **(Jump to) Train your model** â†’ [Model Training Guide](docs/workflow.md#train-model)

---

## How Does H-DaC Work?
**Understanding the detection methods:**

The tool uses **two complementary detection methods** to identify HTR events:

| *Ear Detector Approach* | *Head Detector Approach* |
|:--:|:--:|
| <img src="docs/screenshots/ear_detection_graphic.png" width="400"> | <img src="docs/screenshots/head_detection_graphic.png" width="400"> |
| *Real Ear Example* | *Real Head Example* |
| <img src="docs/screenshots/ear_detect.png" width="400"> | <img src="docs/screenshots/head_detect.png" width="400"> |

**Event Confidence Levels:**
- **Combined (Green)**: Detected by both methods â†’ High confidence
- **Ear Only (Orange)**: Detected by ear method only â†’ Medium confidence
- **Head Only (Red)**: Detected by head method only â†’ Medium confidence

Events are classified using a trained XGBoost model that learns from user-labeled ground truth data.

ðŸ‘‰ **[Read detailed methods documentation â†’](docs/detection_methods.md)**

### Need Help?
- **â“ Common issues and solutions** â†’ [FAQ & Troubleshooting](docs/faq.md)
- **ðŸ› Report a bug** â†’ [GitHub Issues](https://github.com/GraysonButcher/Headtwitch_Detection_and_Classification/issues)
- **ðŸ’¬ Ask questions** â†’ [GitHub Discussions](https://github.com/GraysonButcher/Headtwitch_Detection_and_Classification/discussions)

---

## Documentation

Complete documentation available in the [`/docs`](docs/) directory:

| Guide | Status | Description |
|-------|--------|-------------|
| **[Setup Guide](docs/setup_guide.md)** | âœ… Available | Hardware setup, camera configuration, SLEAP tracking |
| **[Detection Methods](docs/detection_methods.md)** | âœ… Available | Detailed explanation of dual detection methods |
| **[Workflow Guide](docs/workflow.md)** | âœ… Available | Complete end-to-end workflow with decision points |
| **[Installation Guide](docs/installation.md)** | ðŸš§ Coming Soon | Detailed software installation options and troubleshooting |
| **[Parameter Tuning Guide](docs/parameter_tuning_guide.md)** | ðŸš§ Coming Soon | Understanding and adjusting detection parameters |
| **[FAQ & Troubleshooting](docs/faq.md)** | ðŸš§ Coming Soon | Common questions and issues |

ðŸ“¹ **Video Tutorials** - Coming soon!

---

## Configuration

### Node Mapping

SLEAP exports can have different node orderings. Configure via **Settings > Configure Node Mapping**.

Common formats:
- Format 1: Left Ear=0, Right Ear=1, Back=2, Nose=3, Head=4
- Format 2: Nose=0, Head=1, Left Ear=2, Right Ear=3, Back=4

### Parameter Files

Detection parameters can be saved and loaded as JSON files:
- **File > Export Parameters**: Save current parameter configuration
- **File > Import Parameters**: Load saved parameter configuration

---

## Citation

If you use this tool in your research, please cite:

```bibtex
@software{htr_analysis_tool_2025,
  author = {Butcher, Grayson},
  title = {Head-Twitch Response Detection and Classification Tool},
  year = {2025},
  url = {https://github.com/GraysonButcher/Headtwitch_Detection_and_Classification}
}
```

---

## License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please feel free to:
- Report bugs or request features via [GitHub Issues](https://github.com/GraysonButcher/Headtwitch_Detection_and_Classification/issues)
- Submit pull requests with improvements
- Share your parameter configurations or training data

---

**Status**: Active Development | **Version**: 3.0.0 | **Last Updated**: January 2025
